{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b0a2abd8",
      "metadata": {
        "id": "b0a2abd8"
      },
      "source": [
        "\n",
        "# Challenge de Reproducibilidad: MR-DBSCAN\n",
        "## Algoritmo Paralelo de Clustering por Densidad usando MapReduce\n",
        "\n",
        "---\n",
        "\n",
        "## Informaci√≥n del Proyecto\n",
        "\n",
        "**Paper:** MR-DBSCAN: An Efficient Parallel Density-based Clustering Algorithm using MapReduce  \n",
        "**Autores:** Yaobin He, Haoyu Tan, Wuman Luo, et al. (Shenzhen Institutes of Advanced Technology)  \n",
        "**Dataset:** 1.9 billion GPS records (Shanghai taxi data) o sint√©tico  \n",
        "**Plataforma:** Hadoop + PySpark  \n",
        "\n",
        "**Objetivo:** Reproducir el algoritmo MR-DBSCAN en 4 etapas usando MapReduce para clustering de datos espaciales a gran escala.\n",
        "\n",
        "---\n",
        "\n",
        "## Par√°metros del Algoritmo\n",
        "\n",
        "- **Eps (epsilon):** Radio m√°ximo de vecindad\n",
        "  - Limpieza de datos: 0.002 (elimina ruido/outliers)\n",
        "  - Identificaci√≥n de regiones densas: 0.0002 (clusters m√°s espec√≠ficos)\n",
        "\n",
        "- **MinPts:** N√∫mero m√≠nimo de puntos para formar un cluster\n",
        "  - Limpieza: 1000 puntos\n",
        "  - Regiones densas: 100 puntos\n",
        "\n",
        "- **Particiones:** Grid-based partitioning\n",
        "  - Strips por dimensi√≥n: 80-160 seg√∫n Eps\n",
        "\n",
        "---\n",
        "\n",
        "## Divisi√≥n de Tareas por Persona\n",
        "\n",
        "| Rol | Responsabilidades |\n",
        "|-----|-------------------|\n",
        "| **Persona 1** | Etapa 1 (Preprocesamiento y Particionado) + Setup inicial |\n",
        "| **Persona 2** | Etapa 2 (DBSCAN Local) + Etapa 3 (Detecci√≥n de Cruces) |\n",
        "| **Persona 3** | Etapa 4 (Fusi√≥n Global) + Validaci√≥n y Reportes |\n",
        "\n",
        "---\n",
        "\n",
        "## Fases del Algoritmo MR-DBSCAN\n",
        "\n",
        "El algoritmo se ejecuta en 4 etapas usando MapReduce:\n",
        "\n",
        "1. **Stage 1 - Preprocesamiento:** An√°lisis del dataset y particionamiento en grid\n",
        "2. **Stage 2 - DBSCAN Local:** Clustering independiente en cada partici√≥n\n",
        "3. **Stage 3 - Detecci√≥n de Cruces:** Identificar clusters que cruzan fronteras (MC Sets)\n",
        "4. **Stage 4 - Fusi√≥n Global:** Unificar clusters y relabeling global\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e553d9e9",
      "metadata": {
        "id": "e553d9e9"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "# PARTE 1: SETUP INICIAL Y CARGA DE DATOS\n",
        "## Responsable: Jose\n",
        "\n",
        "### Paso 1.1: Configuraci√≥n del Entorno Spark\n",
        "\n",
        "**¬øQu√© hace este paso?**\n",
        "\n",
        "Este paso inicializa una sesi√≥n de Spark (SparkSession), que es el punto de entrada principal para trabajar con DataFrames y RDDs en PySpark. La sesi√≥n establece la conexi√≥n con el cluster Hadoop y configura el nombre de la aplicaci√≥n para identificarla en el Hadoop NameNode.\n",
        "\n",
        "**Detalles t√©cnicos:**\n",
        "- `SparkSession.builder` permite crear una nueva sesi√≥n de forma fluida\n",
        "- `.appName()` asigna el nombre \"MR-DBSCAN\" que ver√°s en la interfaz Hadoop\n",
        "- `.getOrCreate()` obtiene una sesi√≥n existente o crea una nueva\n",
        "\n",
        "**¬øPor qu√© es importante?**\n",
        "- Necesario para distribuir datos y c√≥digo a los nodos del cluster\n",
        "- Permite usar funciones paralelas de Spark en todo el c√≥digo\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "295b7569",
      "metadata": {
        "id": "295b7569"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Inicializar SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "def get_spark_session(app_name='MR-DBSCAN'):\n",
        "    \"\"\"\n",
        "    Crea una nueva sesi√≥n de Spark o retorna la existente.\n",
        "\n",
        "    Par√°metros:\n",
        "        app_name (str): Nombre de la aplicaci√≥n para identificar en Hadoop\n",
        "\n",
        "    Retorna:\n",
        "        SparkSession: Sesi√≥n de Spark lista para usar\n",
        "    \"\"\"\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(app_name) \\\n",
        "        .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    # Configurar nivel de logging para reducir mensajes\n",
        "    spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "    return spark\n",
        "\n",
        "# Crear sesi√≥n\n",
        "spark = get_spark_session()\n",
        "print(f\"‚úì Sesi√≥n de Spark creada exitosamente\")\n",
        "print(f\"  - Spark Version: {spark.version}\")\n",
        "print(f\"  - Master: {spark.sparkContext.master()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32805cf3",
      "metadata": {
        "id": "32805cf3"
      },
      "source": [
        "\n",
        "### üìå Paso 1.2: Cargar y Preprocesar los Datos\n",
        "\n",
        "**¬øQu√© hace este paso?**\n",
        "\n",
        "Este paso:\n",
        "1. **Carga o genera datos:** Si tienes un archivo CSV con GPS, lo carga. Si no, genera datos sint√©ticos que simulan coordenadas GPS (latitud y longitud)\n",
        "2. **Normalizaci√≥n:** Transforma las coordenadas a un rango est√°ndar (0-1) para facilitar el particionado\n",
        "3. **Estructuras de datos:** Convierte los datos a un DataFrame de Pandas (para pre-procesamiento local) o Spark (para procesamiento distribuido)\n",
        "\n",
        "**¬øPor qu√© es importante?**\n",
        "- El MR-DBSCAN requiere datos espaciales (lat, lon) en formato limpio\n",
        "- La normalizaci√≥n es crucial para que el grid partitioning funcione correctamente\n",
        "- Los datos limpios y bien estructurados son la base del algoritmo\n",
        "\n",
        "**Datos de ejemplo:**\n",
        "- Dataset original: 1.9 mil millones de GPS del taxi de Shanghai\n",
        "- Para este ejemplo: 100,000 puntos sint√©ticos distribuidos aleatoriamente\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "905f6c83",
      "metadata": {
        "id": "905f6c83"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Opci√≥n 1: Si tienes un CSV con datos GPS, descomenta y modifica:\n",
        "# data = pd.read_csv('ruta/a/tu/archivo.csv')\n",
        "# data = data[['lon', 'lat']].copy()\n",
        "\n",
        "# Opci√≥n 2: Generar datos sint√©ticos (para demostraci√≥n)\n",
        "n_points = 100000  # N√∫mero de puntos a generar\n",
        "np.random.seed(42)  # Para reproducibilidad\n",
        "\n",
        "# Simular coordenadas GPS de Shanghai (rango real)\n",
        "data = pd.DataFrame({\n",
        "    'lon': np.random.uniform(121.0, 122.0, size=n_points),  # Longitud\n",
        "    'lat': np.random.uniform(30.9, 31.5, size=n_points)    # Latitud\n",
        "})\n",
        "\n",
        "# Normalizar coordenadas al rango [0, 1]\n",
        "data['lon_norm'] = (data['lon'] - data['lon'].min()) / (data['lon'].max() - data['lon'].min())\n",
        "data['lat_norm'] = (data['lat'] - data['lat'].min()) / (data['lat'].max() - data['lat'].min())\n",
        "\n",
        "print(f\"‚úì Datos cargados exitosamente\")\n",
        "print(f\"  - N√∫mero de puntos: {len(data):,}\")\n",
        "print(f\"  - Rango Longitud: [{data['lon'].min():.6f}, {data['lon'].max():.6f}]\")\n",
        "print(f\"  - Rango Latitud: [{data['lat'].min():.6f}, {data['lat'].max():.6f}]\")\n",
        "print(f\"\\nPrimeros 5 registros:\")\n",
        "print(data.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb83ee98",
      "metadata": {
        "id": "cb83ee98"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "# PARTE 2: PARTICIONADO EN GRID\n",
        "## Responsable: Jose\n",
        "\n",
        "### Paso 2.1: Divisi√≥n del Dominio Espacial con Grid\n",
        "\n",
        "**¬øQu√© hace este paso?**\n",
        "\n",
        "Este es el coraz√≥n del Stage 1 del MR-DBSCAN. El particionamiento en grid:\n",
        "\n",
        "1. **Divide el espacio:** Crea una malla 2D que cubre todo el √°rea geogr√°fica\n",
        "2. **Asigna particiones:** Cada punto GPS recibe un ID de partici√≥n seg√∫n su ubicaci√≥n en la malla\n",
        "3. **Equilibra carga:** Intenta que cada partici√≥n tenga aproximadamente la misma cantidad de datos\n",
        "\n",
        "**Conceptos clave:**\n",
        "- **n_strips:** N√∫mero de divisiones por dimensi√≥n (80-160 seg√∫n el paper)\n",
        "  - M√°s strips = particiones m√°s peque√±as (mejor balance pero m√°s r√©plicas en bordes)\n",
        "  - Menos strips = particiones m√°s grandes (menos comunicaci√≥n pero desbalance)\n",
        "- **lon_bin / lat_bin:** √çndice de la celda en cada dimensi√≥n\n",
        "- **partition_id:** Identificador √∫nico de la partici√≥n (combinaci√≥n de lon_bin y lat_bin)\n",
        "\n",
        "**¬øPor qu√© es importante?**\n",
        "- MapReduce distribuye datos a diferentes nodos seg√∫n la partici√≥n\n",
        "- El balance de carga afecta directamente el rendimiento paralelo\n",
        "- Un particionamiento pobre causa \"data skew\" (algunos nodos reciben m√°s datos)\n",
        "\n",
        "**Ejemplo visual:**\n",
        "```\n",
        "Si n_strips=3, espacio 0-1 se divide en:\n",
        "[0-0.33, 0.33-0.67, 0.67-1.0]\n",
        "\n",
        "Un punto en (0.5, 0.8) recibe:\n",
        "lon_bin = 1 (est√° en [0.33-0.67])\n",
        "lat_bin = 2 (est√° en [0.67-1.0])\n",
        "partition_id = \"1_2\"\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f91f53b",
      "metadata": {
        "id": "0f91f53b"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Definir par√°metros de particionamiento\n",
        "n_strips = 120  # √ìptimo seg√∫n el paper para Eps grande (0.002)\n",
        "\n",
        "# Crear bins para dividir el espacio normalizado [0, 1]\n",
        "lon_bins = np.linspace(0, 1, n_strips + 1)\n",
        "lat_bins = np.linspace(0, 1, n_strips + 1)\n",
        "\n",
        "# Asignar cada punto a su bin (celda del grid)\n",
        "data['lon_bin'] = np.digitize(data['lon_norm'], lon_bins) - 1\n",
        "data['lat_bin'] = np.digitize(data['lat_norm'], lat_bins) - 1\n",
        "\n",
        "# Crear identificador √∫nico de partici√≥n\n",
        "data['partition_id'] = data['lon_bin'].astype(str) + '_' + data['lat_bin'].astype(str)\n",
        "\n",
        "# Informaci√≥n del particionado\n",
        "n_partitions = len(data['partition_id'].unique())\n",
        "print(f\"‚úì Particionamiento completado\")\n",
        "print(f\"  - N√∫mero de strips por dimensi√≥n: {n_strips}\")\n",
        "print(f\"  - Total de particiones: {n_partitions}\")\n",
        "print(f\"  - Puntos por partici√≥n (promedio): {len(data) / n_partitions:.1f}\")\n",
        "print(f\"  - Particiones con datos: {(data['partition_id'].value_counts() > 0).sum()}\")\n",
        "\n",
        "# Mostrar distribuci√≥n de datos entre particiones\n",
        "print(f\"\\nDistribuci√≥n de datos:\")\n",
        "partition_counts = data['partition_id'].value_counts()\n",
        "print(f\"  - Partici√≥n con m√°s datos: {partition_counts.max()} puntos\")\n",
        "print(f\"  - Partici√≥n con menos datos: {partition_counts.min()} puntos\")\n",
        "print(f\"  - Desviaci√≥n est√°ndar: {partition_counts.std():.1f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a541d23",
      "metadata": {
        "id": "9a541d23"
      },
      "source": [
        "\n",
        "### Paso 2.2: Preparaci√≥n de Datos para MapReduce\n",
        "\n",
        "**¬øQu√© hace este paso?**\n",
        "\n",
        "Prepara los datos para ser procesados en paralelo en Hadoop:\n",
        "\n",
        "1. **Agrupa por partici√≥n:** Organiza todos los puntos que pertenecen a la misma partici√≥n\n",
        "2. **Convierte a Spark DataFrame:** Transforma los datos de Pandas a Spark para procesamiento distribuido\n",
        "3. **Repartici√≥n:** Distribuye las particiones entre los nodos del cluster Hadoop\n",
        "\n",
        "**¬øPor qu√© es importante?**\n",
        "- MapReduce requiere datos en formato distribuido\n",
        "- Spark DataFrame permite operaciones paralelas autom√°ticas\n",
        "- La repartici√≥n asegura que cada nodo reciba trabajo similar\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49dd50d9",
      "metadata": {
        "id": "49dd50d9"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Opci√≥n 1: Mantener en Pandas para procesamiento local\n",
        "partition_groups = dict(list(data.groupby('partition_id')))\n",
        "print(f\"‚úì Datos agrupados por partici√≥n\")\n",
        "print(f\"  - N√∫mero de grupos: {len(partition_groups)}\")\n",
        "print(f\"  - Ejemplo de keys: {list(partition_groups.keys())[:5]}\")\n",
        "\n",
        "# Opci√≥n 2: Convertir a Spark DataFrame para procesamiento distribuido\n",
        "spark_df = spark.createDataFrame(data)\n",
        "spark_df = spark_df.repartition('partition_id')  # Distribuir por partition_id\n",
        "\n",
        "print(f\"\\n‚úì Spark DataFrame creado\")\n",
        "print(f\"  - N√∫mero de particiones Spark: {spark_df.rdd.getNumPartitions()}\")\n",
        "print(f\"  - Total de registros: {spark_df.count():,}\")\n",
        "print(f\"\\nEstructura de datos:\")\n",
        "spark_df.printSchema()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}