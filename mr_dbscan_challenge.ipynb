{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0a2abd8",
   "metadata": {
    "id": "b0a2abd8"
   },
   "source": [
    "\n",
    "# Challenge de Reproducibilidad: MR-DBSCAN\n",
    "## Algoritmo Paralelo de Clustering por Densidad usando MapReduce\n",
    "\n",
    "---\n",
    "\n",
    "## EJECUCI√ìN EN CLUSTER HADOOP\n",
    "\n",
    "**Configuraci√≥n requerida:**\n",
    "- Jupyter Lab ejecut√°ndose en el nodo maestro\n",
    "- Spark configurado para usar YARN (cluster mode)\n",
    "- Hadoop HDFS disponible para almacenamiento distribuido\n",
    "- Todas las dependencias (pyspark, numpy, pandas, scikit-learn) instaladas en todos los nodos\n",
    "\n",
    "---\n",
    "\n",
    "## Informaci√≥n del Proyecto\n",
    "\n",
    "**Paper:** MR-DBSCAN: An Efficient Parallel Density-based Clustering Algorithm using MapReduce  \n",
    "**Autores:** Yaobin He, Haoyu Tan, Wuman Luo, et al. (Shenzhen Institutes of Advanced Technology)  \n",
    "**Dataset:** Datos GPS reales de taxis de Shanghai (carpeta Taxi_070220)  \n",
    "**Plataforma:** Hadoop + PySpark  \n",
    "\n",
    "**Objetivo:** Reproducir el algoritmo MR-DBSCAN en 4 etapas usando MapReduce para clustering de datos espaciales a gran escala.\n",
    "\n",
    "---\n",
    "\n",
    "## Par√°metros del Algoritmo\n",
    "\n",
    "- **Eps (epsilon):** Radio m√°ximo de vecindad\n",
    "  - Limpieza de datos: 0.002 (elimina ruido/outliers)\n",
    "  - Identificaci√≥n de regiones densas: 0.0002 (clusters m√°s espec√≠ficos)\n",
    "\n",
    "- **MinPts:** N√∫mero m√≠nimo de puntos para formar un cluster\n",
    "  - Limpieza: 1000 puntos\n",
    "  - Regiones densas: 100 puntos\n",
    "\n",
    "- **Particiones:** Grid-based partitioning\n",
    "  - Strips por dimensi√≥n: 80-160 seg√∫n Eps\n",
    "\n",
    "---\n",
    "\n",
    "## Divisi√≥n de Tareas por Persona\n",
    "\n",
    "| Rol | Responsabilidades |\n",
    "|-----|-------------------|\n",
    "| **Jose** | Etapa 1 (Preprocesamiento y Particionado) + Setup inicial |\n",
    "| **Miguel** | Etapa 2 (DBSCAN Local) + Etapa 3 (Detecci√≥n de Cruces) |\n",
    "| **Roger** | Etapa 4 (Fusi√≥n Global) + Validaci√≥n y Reportes |\n",
    "\n",
    "---\n",
    "\n",
    "## Fases del Algoritmo MR-DBSCAN\n",
    "\n",
    "El algoritmo se ejecuta en 4 etapas usando MapReduce:\n",
    "\n",
    "1. **Stage 1 - Preprocesamiento:** An√°lisis del dataset y particionamiento en grid\n",
    "2. **Stage 2 - DBSCAN Local:** Clustering independiente en cada partici√≥n\n",
    "3. **Stage 3 - Detecci√≥n de Cruces:** Identificar clusters que cruzan fronteras (MC Sets)\n",
    "4. **Stage 4 - Fusi√≥n Global:** Unificar clusters y relabeling global\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e553d9e9",
   "metadata": {
    "id": "e553d9e9"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "# PARTE 1: SETUP INICIAL Y CARGA DE DATOS\n",
    "## Responsable: Jose\n",
    "\n",
    "### Paso 1.1: Configuraci√≥n del Entorno Spark\n",
    "\n",
    "**¬øQu√© hace este paso?**\n",
    "\n",
    "Este paso inicializa una sesi√≥n de Spark (SparkSession), que es el punto de entrada principal para trabajar con DataFrames y RDDs en PySpark. La sesi√≥n establece la conexi√≥n con el cluster Hadoop y configura el nombre de la aplicaci√≥n para identificarla en el Hadoop NameNode.\n",
    "\n",
    "**Detalles t√©cnicos:**\n",
    "- `SparkSession.builder` permite crear una nueva sesi√≥n de forma fluida\n",
    "- `.appName()` asigna el nombre \"MR-DBSCAN\" que ver√°s en la interfaz Hadoop\n",
    "- `.getOrCreate()` obtiene una sesi√≥n existente o crea una nueva\n",
    "\n",
    "**¬øPor qu√© es importante?**\n",
    "- Necesario para distribuir datos y c√≥digo a los nodos del cluster\n",
    "- Permite usar funciones paralelas de Spark en todo el c√≥digo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "295b7569",
   "metadata": {
    "id": "295b7569"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/10 18:58:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Sesi√≥n de Spark creada exitosamente\n",
      "  - Spark Version: 3.5.6\n",
      "  - Master: local[*]\n",
      "  - Aplicaci√≥n: MR-DBSCAN\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Inicializar SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "def get_spark_session(app_name='MR-DBSCAN'):\n",
    "    \"\"\"\n",
    "    Crea una nueva sesi√≥n de Spark o retorna la existente.\n",
    "\n",
    "    Par√°metros:\n",
    "        app_name (str): Nombre de la aplicaci√≥n para identificar en Hadoop\n",
    "\n",
    "    Retorna:\n",
    "        SparkSession: Sesi√≥n de Spark lista para usar\n",
    "    \"\"\"\n",
    "    # Configuraci√≥n para ejecuci√≥n en cluster Hadoop\n",
    "    # Si est√°s en modo local, Spark detectar√° autom√°ticamente\n",
    "    # Si est√°s en cluster, debe estar configurado con master=\"yarn\"\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "        .config(\"spark.executor.memory\", \"2g\") \\\n",
    "        .config(\"spark.executor.cores\", \"2\") \\\n",
    "        .config(\"spark.driver.memory\", \"1g\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Configurar nivel de logging para reducir mensajes\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "    return spark\n",
    "\n",
    "# Crear sesi√≥n\n",
    "spark = get_spark_session()\n",
    "print(f\"‚úì Sesi√≥n de Spark creada exitosamente\")\n",
    "print(f\"  - Spark Version: {spark.version}\")\n",
    "print(f\"  - Master: {spark.sparkContext.master}\")\n",
    "print(f\"  - Aplicaci√≥n: {spark.sparkContext.appName}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32805cf3",
   "metadata": {
    "id": "32805cf3"
   },
   "source": [
    "\n",
    "### üìå Paso 1.2: Cargar y Preprocesar los Datos\n",
    "\n",
    "**¬øQu√© hace este paso?**\n",
    "\n",
    "Este paso:\n",
    "1. **Verifica CSV unificado:** Busca el archivo `taxi_data_unificado.csv` (generado por `unificar_dataset.py`)\n",
    "2. **Carga datos:** Si existe el CSV, lo lee directamente (m√°s r√°pido). Si no existe, lo genera autom√°ticamente.\n",
    "3. **Limpieza de datos:** Elimina valores nulos e inv√°lidos y filtra coordenadas fuera del rango v√°lido\n",
    "4. **Normalizaci√≥n:** Transforma las coordenadas a un rango est√°ndar (0-1) para facilitar el particionado\n",
    "5. **Estructuras de datos:** Convierte los datos a un DataFrame de Pandas (para pre-procesamiento local) o Spark (para procesamiento distribuido)\n",
    "\n",
    "**¬øPor qu√© es importante?**\n",
    "- El MR-DBSCAN requiere datos espaciales (lat, lon) en formato limpio\n",
    "- La normalizaci√≥n es crucial para que el grid partitioning funcione correctamente\n",
    "- Los datos limpios y bien estructurados son la base del algoritmo\n",
    "- Usar un CSV unificado evita tener que leer miles de archivos en cada ejecuci√≥n\n",
    "\n",
    "**Dataset:**\n",
    "- Fuente: Datos GPS reales de taxis de Shanghai (carpeta Taxi_070220)\n",
    "- Formato: Archivo CSV unificado `taxi_data_unificado.csv` con columnas: lon, lat\n",
    "- Generaci√≥n: El CSV se genera ejecutando `python unificar_dataset.py`.\n",
    "- Columnas utilizadas: longitud (lon) y latitud (lat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "905f6c83",
   "metadata": {
    "id": "905f6c83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Archivo CSV unificado encontrado: taxi_data_unificado.csv\n",
      " Cargando datos desde CSV...\n",
      " Datos cargados desde CSV\n",
      "  - Total de puntos: 49,662\n",
      "\n",
      " Validando y limpiando datos...\n",
      "  - Todas las filas son v√°lidas: 49,662\n",
      "\n",
      " Normalizando coordenadas...\n",
      "\n",
      "======================================================================\n",
      " PREPROCESAMIENTO COMPLETADO\n",
      "======================================================================\n",
      "  - Total de puntos: 49,662\n",
      "  - Rango Longitud: [121.148300, 121.866100]\n",
      "  - Rango Latitud: [30.886600, 31.999100]\n",
      "  - Coordenadas normalizadas al rango [0, 1]\n",
      "======================================================================\n",
      "\n",
      "Primeros 5 registros:\n",
      "        lon      lat  lon_norm  lat_norm\n",
      "0  121.4666  31.2208  0.443438  0.300404\n",
      "1  121.4681  31.2211  0.445528  0.300674\n",
      "2  121.4695  31.2216  0.447478  0.301124\n",
      "3  121.4700  31.2216  0.448175  0.301124\n",
      "4  121.4695  31.2215  0.447478  0.301034\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Nombre del archivo CSV unificado\n",
    "CSV_UNIFICADO = 'taxi_data_unificado.csv'\n",
    "CARPETA_DATOS = 'Taxi_070220'\n",
    "\n",
    "# Cargar datos desde CSV unificado\n",
    "csv_path = Path(CSV_UNIFICADO)\n",
    "data_folder = Path(CARPETA_DATOS)\n",
    "\n",
    "# Verificar si el CSV unificado existe\n",
    "if csv_path.exists():\n",
    "    print(f\" Archivo CSV unificado encontrado: {CSV_UNIFICADO}\")\n",
    "    print(f\" Cargando datos desde CSV...\")\n",
    "    \n",
    "    # Leer el CSV unificado\n",
    "    data = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Verificar que tiene las columnas correctas\n",
    "    if 'lon' not in data.columns or 'lat' not in data.columns:\n",
    "        raise ValueError(f\"El archivo {CSV_UNIFICADO} no tiene las columnas 'lon' y 'lat'\")\n",
    "    \n",
    "    print(f\" Datos cargados desde CSV\")\n",
    "    print(f\"  - Total de puntos: {len(data):,}\"),\n",
    "\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        f\"El archivo {CSV_UNIFICADO} no existe. \"\n",
    "        f\"Ejecuta primero: python unificar_dataset.py\"\n",
    "    )\n",
    "\n",
    "# Limpieza y validaci√≥n de datos\n",
    "print(f\"\\n Validando y limpiando datos...\")\n",
    "\n",
    "initial_count = len(data)\n",
    "\n",
    "# Eliminar valores nulos\n",
    "data = data.dropna(subset=['lon', 'lat'])\n",
    "\n",
    "# Filtrar coordenadas inv√°lidas (rango v√°lido para GPS de Shanghai)\n",
    "data = data[(data['lon'] >= 120) & (data['lon'] <= 122)]\n",
    "data = data[(data['lat'] >= 30) & (data['lat'] <= 32)]\n",
    "\n",
    "if len(data) < initial_count:\n",
    "    removed = initial_count - len(data)\n",
    "    print(f\"  - Filas eliminadas (valores inv√°lidos): {removed:,}\")\n",
    "    print(f\"  - Filas v√°lidas: {len(data):,}\")\n",
    "else:\n",
    "    print(f\"  - Todas las filas son v√°lidas: {len(data):,}\")\n",
    "\n",
    "# Normalizaci√≥n de coordenadas\n",
    "print(f\"\\n Normalizando coordenadas...\")\n",
    "\n",
    "# Guardar los rangos originales para posible desnormalizaci√≥n\n",
    "lon_min, lon_max = data['lon'].min(), data['lon'].max()\n",
    "lat_min, lat_max = data['lat'].min(), data['lat'].max()\n",
    "\n",
    "lon_range = lon_max - lon_min\n",
    "lat_range = lat_max - lat_min\n",
    "\n",
    "# Verificar que hay rango v√°lido (evitar divisi√≥n por cero)\n",
    "if lon_range > 0:\n",
    "    data['lon_norm'] = (data['lon'] - lon_min) / lon_range\n",
    "else:\n",
    "    print(\" Advertencia: Todos los puntos tienen la misma longitud\")\n",
    "    data['lon_norm'] = 0.5\n",
    "\n",
    "if lat_range > 0:\n",
    "    data['lat_norm'] = (data['lat'] - lat_min) / lat_range\n",
    "else:\n",
    "    print(\" Advertencia: Todos los puntos tienen la misma latitud\")\n",
    "    data['lat_norm'] = 0.5\n",
    "\n",
    "# Resumen final\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(f\" PREPROCESAMIENTO COMPLETADO\")\n",
    "print(f\"=\" * 70)\n",
    "print(f\"  - Total de puntos: {len(data):,}\")\n",
    "print(f\"  - Rango Longitud: [{lon_min:.6f}, {lon_max:.6f}]\")\n",
    "print(f\"  - Rango Latitud: [{lat_min:.6f}, {lat_max:.6f}]\")\n",
    "print(f\"  - Coordenadas normalizadas al rango [0, 1]\")\n",
    "print(f\"=\" * 70)\n",
    "\n",
    "print(f\"\\nPrimeros 5 registros:\")\n",
    "print(data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb83ee98",
   "metadata": {
    "id": "cb83ee98"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "# PARTE 2: PARTICIONADO EN GRID\n",
    "## Responsable: Jose\n",
    "\n",
    "### Paso 2.1: Divisi√≥n del Dominio Espacial con Grid\n",
    "\n",
    "**¬øQu√© hace este paso?**\n",
    "\n",
    "Este es el coraz√≥n del Stage 1 del MR-DBSCAN. El particionamiento en grid:\n",
    "\n",
    "1. **Divide el espacio:** Crea una malla 2D que cubre todo el √°rea geogr√°fica\n",
    "2. **Asigna particiones:** Cada punto GPS recibe un ID de partici√≥n seg√∫n su ubicaci√≥n en la malla\n",
    "3. **Equilibra carga:** Intenta que cada partici√≥n tenga aproximadamente la misma cantidad de datos\n",
    "\n",
    "**Conceptos clave:**\n",
    "- **n_strips:** N√∫mero de divisiones por dimensi√≥n (80-160 seg√∫n el paper)\n",
    "  - M√°s strips = particiones m√°s peque√±as (mejor balance pero m√°s r√©plicas en bordes)\n",
    "  - Menos strips = particiones m√°s grandes (menos comunicaci√≥n pero desbalance)\n",
    "- **lon_bin / lat_bin:** √çndice de la celda en cada dimensi√≥n\n",
    "- **partition_id:** Identificador √∫nico de la partici√≥n (combinaci√≥n de lon_bin y lat_bin)\n",
    "\n",
    "**¬øPor qu√© es importante?**\n",
    "- MapReduce distribuye datos a diferentes nodos seg√∫n la partici√≥n\n",
    "- El balance de carga afecta directamente el rendimiento paralelo\n",
    "- Un particionamiento pobre causa \"data skew\" (algunos nodos reciben m√°s datos)\n",
    "\n",
    "**Ejemplo visual:**\n",
    "```\n",
    "Si n_strips=3, espacio 0-1 se divide en:\n",
    "[0-0.33, 0.33-0.67, 0.67-1.0]\n",
    "\n",
    "Un punto en (0.5, 0.8) recibe:\n",
    "lon_bin = 1 (est√° en [0.33-0.67])\n",
    "lat_bin = 2 (est√° en [0.67-1.0])\n",
    "partition_id = \"1_2\"\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f91f53b",
   "metadata": {
    "id": "0f91f53b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Particionamiento completado\n",
      "  - N√∫mero de strips por dimensi√≥n: 120\n",
      "  - Total de particiones √∫nicas con datos: 1873\n",
      "  - Total de particiones posibles en grid: 14400\n",
      "  - Puntos por partici√≥n (promedio): 26.5\n",
      "\n",
      "Distribuci√≥n de datos:\n",
      "  - Partici√≥n con m√°s datos: 1318 puntos\n",
      "  - Partici√≥n con menos datos: 1 puntos\n",
      "  - Desviaci√≥n est√°ndar: 69.2\n",
      "\n",
      " Informaci√≥n de bins guardada para detecci√≥n de fronteras\n",
      "  - L√≠mites longitud: [0.000000, 1.000000]\n",
      "  - L√≠mites latitud: [0.000000, 1.000000]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Definir par√°metros de particionamiento\n",
    "# Guardar n_strips como variable global para usar en detecci√≥n de fronteras\n",
    "n_strips = 120  # √ìptimo seg√∫n el paper para Eps grande (0.002)\n",
    "\n",
    "# Crear bins para dividir el espacio normalizado [0, 1]\n",
    "# np.linspace crea n_strips+1 l√≠mites, generando n_strips intervalos\n",
    "lon_bins = np.linspace(0, 1, n_strips + 1)\n",
    "lat_bins = np.linspace(0, 1, n_strips + 1)\n",
    "\n",
    "# np.digitize con right=False para manejar correctamente los bordes\n",
    "# Los puntos exactamente en el l√≠mite superior deben ir al bin anterior\n",
    "# Usamos np.searchsorted que es m√°s preciso para este caso\n",
    "data['lon_bin'] = np.searchsorted(lon_bins, data['lon_norm'], side='right') - 1\n",
    "data['lat_bin'] = np.searchsorted(lat_bins, data['lat_norm'], side='right') - 1\n",
    "\n",
    "# Asegurar que los bins est√©n en el rango v√°lido [0, n_strips-1]\n",
    "# Los puntos en el l√≠mite superior (1.0) pueden dar n_strips, los ajustamos\n",
    "data['lon_bin'] = np.clip(data['lon_bin'], 0, n_strips - 1)\n",
    "data['lat_bin'] = np.clip(data['lat_bin'], 0, n_strips - 1)\n",
    "\n",
    "# Crear identificador √∫nico de partici√≥n\n",
    "data['partition_id'] = data['lon_bin'].astype(str) + '_' + data['lat_bin'].astype(str)\n",
    "\n",
    "# Informaci√≥n del particionado\n",
    "n_partitions = len(data['partition_id'].unique())\n",
    "print(f\" Particionamiento completado\")\n",
    "print(f\"  - N√∫mero de strips por dimensi√≥n: {n_strips}\")\n",
    "print(f\"  - Total de particiones √∫nicas con datos: {n_partitions}\")\n",
    "print(f\"  - Total de particiones posibles en grid: {n_strips * n_strips}\")\n",
    "print(f\"  - Puntos por partici√≥n (promedio): {len(data) / n_partitions:.1f}\")\n",
    "\n",
    "# Mostrar distribuci√≥n de datos entre particiones\n",
    "partition_counts = data['partition_id'].value_counts()\n",
    "print(f\"\\nDistribuci√≥n de datos:\")\n",
    "print(f\"  - Partici√≥n con m√°s datos: {partition_counts.max()} puntos\")\n",
    "print(f\"  - Partici√≥n con menos datos: {partition_counts.min()} puntos\")\n",
    "print(f\"  - Desviaci√≥n est√°ndar: {partition_counts.std():.1f}\")\n",
    "\n",
    "# Guardar informaci√≥n de bins para usar en detecci√≥n de fronteras\n",
    "# Esto se usar√° despu√©s para determinar los l√≠mites reales de cada celda\n",
    "print(f\"\\n Informaci√≥n de bins guardada para detecci√≥n de fronteras\")\n",
    "print(f\"  - L√≠mites longitud: [{lon_bins[0]:.6f}, {lon_bins[-1]:.6f}]\")\n",
    "print(f\"  - L√≠mites latitud: [{lat_bins[0]:.6f}, {lat_bins[-1]:.6f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a541d23",
   "metadata": {
    "id": "9a541d23"
   },
   "source": [
    "\n",
    "### Paso 2.2: Preparaci√≥n de Datos para MapReduce\n",
    "\n",
    "**¬øQu√© hace este paso?**\n",
    "\n",
    "Prepara los datos para ser procesados en paralelo en Hadoop:\n",
    "\n",
    "1. **Agrupa por partici√≥n:** Organiza todos los puntos que pertenecen a la misma partici√≥n\n",
    "2. **Convierte a Spark DataFrame:** Transforma los datos de Pandas a Spark para procesamiento distribuido\n",
    "3. **Repartici√≥n:** Distribuye las particiones entre los nodos del cluster Hadoop\n",
    "\n",
    "**¬øPor qu√© es importante?**\n",
    "- MapReduce requiere datos en formato distribuido\n",
    "- Spark DataFrame permite operaciones paralelas autom√°ticas\n",
    "- La repartici√≥n asegura que cada nodo reciba trabajo similar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49dd50d9",
   "metadata": {
    "id": "49dd50d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Datos agrupados por partici√≥n (Pandas)\n",
      "  - N√∫mero de grupos: 1873\n",
      "  - Ejemplo de keys: ['0_12', '0_14', '0_45', '100_29', '100_31']\n",
      "\n",
      " Spark DataFrame creado y optimizado para cluster Hadoop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/10 18:58:44 WARN TaskSetManager: Stage 0 contains a task of very large size (2482 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 0:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - N√∫mero de particiones Spark: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/10 18:58:49 WARN TaskSetManager: Stage 1 contains a task of very large size (2482 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Total de registros: 49,662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/10 18:58:57 WARN TaskSetManager: Stage 7 contains a task of very large size (2482 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 9:====================================================>  (191 + 1) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Particiones de datos √∫nicas (grids): 1873\n",
      "\n",
      "Estructura de datos:\n",
      "root\n",
      " |-- lon: double (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon_norm: double (nullable = true)\n",
      " |-- lat_norm: double (nullable = true)\n",
      " |-- lon_bin: long (nullable = true)\n",
      " |-- lat_bin: long (nullable = true)\n",
      " |-- partition_id: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Preparaci√≥n de datos para MapReduce\n",
    "# Opci√≥n 1: Mantener en Pandas para procesamiento local\n",
    "partition_groups = dict(list(data.groupby('partition_id')))\n",
    "print(f\"‚úì Datos agrupados por partici√≥n (Pandas)\")\n",
    "print(f\"  - N√∫mero de grupos: {len(partition_groups)}\")\n",
    "print(f\"  - Ejemplo de keys: {list(partition_groups.keys())[:5]}\")\n",
    "\n",
    "# Opci√≥n 2: Convertir a Spark DataFrame para procesamiento distribuido\n",
    "# Usamos 200 para mejor balance de carga y evitar \"data skew\"\n",
    "n_spark_partitions = 200  # N√∫mero recomendado de particiones Spark\n",
    "\n",
    "try:\n",
    "    # Convertir DataFrame de Pandas a Spark\n",
    "    # Esto permite procesamiento distribuido en el cluster Hadoop\n",
    "    spark_df = spark.createDataFrame(data)\n",
    "    \n",
    "    # Reparticionar por partition_id con n√∫mero √≥ptimo de particiones\n",
    "    # Esto distribuye mejor la carga entre los nodos del cluster y evita warnings\n",
    "    # Cada partici√≥n del grid puede estar en diferentes nodos para paralelismo real\n",
    "    spark_df = spark_df.repartition(n_spark_partitions, 'partition_id')\n",
    "    \n",
    "    # Cachear el DataFrame si se va a usar m√∫ltiples veces\n",
    "    # Para este caso, solo lo usamos para mostrar informaci√≥n, no lo cacheamos\n",
    "    \n",
    "    print(f\"\\n Spark DataFrame creado y optimizado para cluster Hadoop\")\n",
    "    print(f\"  - N√∫mero de particiones Spark: {spark_df.rdd.getNumPartitions()}\")\n",
    "    print(f\"  - Total de registros: {spark_df.count():,}\")\n",
    "    print(f\"  - Particiones de datos √∫nicas (grids): {spark_df.select('partition_id').distinct().count()}\")\n",
    "    print(f\"\\nEstructura de datos:\")\n",
    "    spark_df.printSchema()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n Advertencia: Error al crear Spark DataFrame: {e}\")\n",
    "    print(\"  Continuando con procesamiento local usando Pandas...\")\n",
    "    print(\"  Para ejecuci√≥n en cluster Hadoop, verifica la configuraci√≥n de Spark\")\n",
    "    spark_df = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f38676",
   "metadata": {
    "id": "a4f38676"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "# PARTE 3: DBSCAN LOCAL Y GENERACI√ìN DE MC SETS\n",
    "## Responsable: Miguel\n",
    "\n",
    "### Paso 3.1: DBSCAN Local en Cada Partici√≥n\n",
    "\n",
    "**¬øQu√© hace este paso? (Stage 2 del paper)**\n",
    "\n",
    "Este es el coraz√≥n del clustering. Para cada partici√≥n:\n",
    "\n",
    "1. **Ejecuta DBSCAN localmente:** Aplica el algoritmo cl√°sico DBSCAN a los puntos de la partici√≥n\n",
    "2. **Clasifica puntos:** Etiqueta cada punto como:\n",
    "   - **Core Point (C):** Tiene ‚â• MinPts vecinos dentro de Eps\n",
    "   - **Border Point (B):** Tiene < MinPts vecinos pero es vecino de un Core Point\n",
    "   - **Noise (N):** No cumple ninguna condici√≥n (outlier)\n",
    "3. **Genera clusters locales:** Crea cluster IDs locales por partici√≥n\n",
    "\n",
    "**Par√°metros importantes:**\n",
    "- **Eps = 0.002:** Radio de b√∫squeda (distancia m√°xima a vecinos)\n",
    "- **MinPts = 1000:** M√≠nimo n√∫mero de puntos para ser Core Point\n",
    "\n",
    "**Algoritmo DBSCAN en pseudoc√≥digo:**\n",
    "```\n",
    "Para cada punto p no visitado:\n",
    "  1. Si p tiene < MinPts vecinos ‚Üí marcar como Noise\n",
    "  2. Si p tiene ‚â• MinPts vecinos ‚Üí marcar como Core Point\n",
    "  3. Expandir: recursivamente procesar todos sus vecinos\n",
    "  4. Crear cluster cuando se termina la expansi√≥n\n",
    "```\n",
    "\n",
    "**¬øPor qu√© es importante?**\n",
    "- Cada nodo ejecuta esta operaci√≥n independientemente (paralelismo real)\n",
    "- Los resultados locales se combinan despu√©s en el Stage 4\n",
    "- Reduce significativamente la complejidad computacional general\n",
    "\n",
    "**Complejidad:**\n",
    "- Tiempo: O(n * log n) con indexaci√≥n espacial, O(n¬≤) sin indexaci√≥n\n",
    "- Espacio: O(n) para almacenar el dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9668327c",
   "metadata": {
    "id": "9668327c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Par√°metros de DBSCAN:\n",
      "  - Eps: 0.002\n",
      "  - MinPts: 1000\n",
      "\n",
      " Ejecutando DBSCAN local en 1873 particiones...\n",
      " DBSCAN local completado\n",
      "  - Puntos en clusters: 2,436\n",
      "  - Puntos noise/outliers: 47,226\n",
      "  - Total de clusters locales √∫nicos: 1\n",
      "  - ID m√°ximo de cluster local: 0\n",
      "\n",
      "Estad√≠sticas por partici√≥n:\n",
      "              clusters  total_points  noise\n",
      "partition_id                               \n",
      "0_12                 0             1      1\n",
      "0_14                 0             2      2\n",
      "0_45                 0             2      2\n",
      "100_29               0            11     11\n",
      "100_31               0             8      8\n",
      "100_32               0             7      7\n",
      "100_33               0             8      8\n",
      "100_34               0             2      2\n",
      "101_28               0            13     13\n",
      "101_31               0            13     13\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import KDTree\n",
    "import scipy.spatial.distance as spatial_distance\n",
    "\n",
    "# Definir par√°metros DBSCAN\n",
    "EPS = 0.002  # Radio de b√∫squeda en coordenadas normalizadas\n",
    "MIN_SAMPLES = 1000  # M√≠nimo n√∫mero de puntos para ser Core Point\n",
    "\n",
    "print(f\"Par√°metros de DBSCAN:\")\n",
    "print(f\"  - Eps: {EPS}\")\n",
    "print(f\"  - MinPts: {MIN_SAMPLES}\")\n",
    "\n",
    "def local_dbscan_partition(partition_data, eps=EPS, min_samples=MIN_SAMPLES):\n",
    "    \"\"\"\n",
    "    Ejecuta DBSCAN en una partici√≥n individual.\n",
    "\n",
    "    Par√°metros:\n",
    "        partition_data: DataFrame con puntos de una partici√≥n\n",
    "        eps: Radio de b√∫squeda\n",
    "        min_samples: M√≠nimo de puntos para Core Point\n",
    "\n",
    "    Retorna:\n",
    "        array: Etiquetas de cluster locales para cada punto (-1 = noise)\n",
    "    \"\"\"\n",
    "    coords = partition_data[['lon_norm', 'lat_norm']].values\n",
    "\n",
    "    # Si la partici√≥n tiene pocos puntos, todos son noise\n",
    "    if len(coords) < min_samples:\n",
    "        return np.array([-1] * len(coords))\n",
    "\n",
    "    # Ejecutar DBSCAN\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples, metric='euclidean', n_jobs=-1)\n",
    "    labels = db.fit_predict(coords)\n",
    "\n",
    "    return labels\n",
    "\n",
    "# Aplicar DBSCAN a cada partici√≥n\n",
    "print(f\"\\n Ejecutando DBSCAN local en {n_partitions} particiones...\")\n",
    "\n",
    "data['local_cluster'] = -1  # Inicializar\n",
    "\n",
    "for partition_id, group_idx in data.groupby('partition_id').groups.items():\n",
    "    group = data.loc[group_idx]\n",
    "    labels = local_dbscan_partition(group)\n",
    "    data.loc[group_idx, 'local_cluster'] = labels\n",
    "\n",
    "print(f\" DBSCAN local completado\")\n",
    "print(f\"  - Puntos en clusters: {(data['local_cluster'] != -1).sum():,}\")\n",
    "print(f\"  - Puntos noise/outliers: {(data['local_cluster'] == -1).sum():,}\")\n",
    "\n",
    "# Calcular n√∫mero de clusters locales de forma segura\n",
    "clusters_local = data[data['local_cluster'] != -1]['local_cluster']\n",
    "if len(clusters_local) > 0:\n",
    "    n_local_clusters = clusters_local.nunique()\n",
    "    max_cluster_id = clusters_local.max()\n",
    "    print(f\"  - Total de clusters locales √∫nicos: {n_local_clusters}\")\n",
    "    print(f\"  - ID m√°ximo de cluster local: {max_cluster_id}\")\n",
    "else:\n",
    "    print(f\"  - Total de clusters locales: 0 (todos los puntos son noise)\")\n",
    "    n_local_clusters = 0\n",
    "\n",
    "# Mostrar estad√≠sticas por partici√≥n\n",
    "print(f\"\\nEstad√≠sticas por partici√≥n:\")\n",
    "try:\n",
    "    # Calcular puntos en clusters por partici√≥n\n",
    "    stats = data.groupby('partition_id').agg({\n",
    "        'local_cluster': lambda x: (x != -1).sum(),  # Puntos en clusters\n",
    "    })\n",
    "    stats = stats.rename(columns={'local_cluster': 'clusters'})\n",
    "    # Ahora agregar total_points y noise\n",
    "    stats['total_points'] = data.groupby('partition_id').size()\n",
    "    stats['noise'] = stats['total_points'] - stats['clusters']\n",
    "    print(stats.head(10))\n",
    "except Exception as e:\n",
    "    print(f\" Error al calcular estad√≠sticas: {e}\")\n",
    "    print(\"  Continuando con el siguiente paso...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab45fd84",
   "metadata": {
    "id": "ab45fd84"
   },
   "source": [
    "\n",
    "### Paso 3.2: Identificaci√≥n de Puntos Frontera (MC Sets)\n",
    "\n",
    "**¬øQu√© hace este paso? (Preparaci√≥n para Stage 3)**\n",
    "\n",
    "Identifica puntos especiales que **cruzan fronteras entre particiones**:\n",
    "\n",
    "1. **Puntos frontera:** Puntos cuyo vecindario Eps-distance cruza hacia otra partici√≥n\n",
    "2. **MC Sets (Merge Candidate Sets):** Conjuntos de puntos que potencialmente deben fusionarse\n",
    "3. **Clasificaci√≥n:** Marca los puntos que necesitar√°n procesamiento especial\n",
    "\n",
    "**¬øPor qu√© es importante?**\n",
    "- DBSCAN local NO ve puntos en otras particiones\n",
    "- Dos clusters en particiones vecinas podr√≠an ser el MISMO cluster globalmente\n",
    "- Necesitamos identificar estos casos ANTES de hacer la fusi√≥n\n",
    "\n",
    "**Concepto clave - El problema de las fronteras:**\n",
    "\n",
    "```\n",
    "Partici√≥n A        |  Partici√≥n B\n",
    "    ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè|‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
    "    ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè|‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
    "\n",
    "Local en A: cluster 1\n",
    "Local en B: cluster 1\n",
    "Pero podr√≠an ser el MISMO cluster si un punto de A est√° cerca de B\n",
    "\n",
    "‚Üí Los puntos cerca de la frontera deben compararse entre particiones\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4298fcce",
   "metadata": {
    "id": "4298fcce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Identificando puntos frontera usando l√≠mites de celdas del grid...\n",
      " Puntos frontera identificados\n",
      "  - Total puntos frontera: 34,814 (70.1%)\n",
      "  - Puntos internos: 14,848\n",
      "\n",
      " MC Sets (Merge Candidates) creados\n",
      "  - Total de candidatos para fusi√≥n: 265\n",
      "  - Clusters √∫nicos en MC: 1\n",
      "  - Particiones con candidatos: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Identificar puntos frontera usando l√≠mites reales de la celda del grid\n",
    "def identify_border_points(partition_data, partition_id, lon_bins, lat_bins, eps=EPS):\n",
    "    \"\"\"\n",
    "    Identifica puntos de una partici√≥n que est√°n cerca de las fronteras de su celda del grid.\n",
    "\n",
    "    Par√°metros:\n",
    "        partition_data: DataFrame con puntos de una partici√≥n\n",
    "        partition_id: ID de la partici√≥n (formato \"lon_bin_lat_bin\")\n",
    "        lon_bins: Array con l√≠mites de bins para longitud\n",
    "        lat_bins: Array con l√≠mites de bins para latitud\n",
    "        eps: Radio de b√∫squeda Eps\n",
    "\n",
    "    \"\"\"\n",
    "    coords = partition_data[['lon_norm', 'lat_norm']].values\n",
    "\n",
    "    if len(coords) == 0:\n",
    "        return np.array([], dtype=bool)\n",
    "\n",
    "    # Obtener los l√≠mites reales de la celda del grid\n",
    "    # Parsear partition_id para obtener los √≠ndices de bin\n",
    "    try:\n",
    "        lon_bin_idx, lat_bin_idx = map(int, partition_id.split('_'))\n",
    "    except:\n",
    "        # Si falla el parsing, usar m√©todo alternativo (l√≠mites de datos)\n",
    "        lon_min, lon_max = coords[:, 0].min(), coords[:, 0].max()\n",
    "        lat_min, lat_max = coords[:, 1].min(), coords[:, 1].max()\n",
    "    else:\n",
    "        # Usar los l√≠mites reales de la celda del grid\n",
    "        lon_min = lon_bins[lon_bin_idx]\n",
    "        lon_max = lon_bins[lon_bin_idx + 1] if lon_bin_idx + 1 < len(lon_bins) else lon_bins[-1]\n",
    "        lat_min = lat_bins[lat_bin_idx]\n",
    "        lat_max = lat_bins[lat_bin_idx + 1] if lat_bin_idx + 1 < len(lat_bins) else lat_bins[-1]\n",
    "\n",
    "    # Comprobar si est√° dentro de Eps de cualquier borde de la celda\n",
    "    # Esto asegura que detectamos puntos que pueden conectarse con particiones vecinas\n",
    "    is_border = (\n",
    "        (coords[:, 0] <= lon_min + eps) |  # Cerca borde oeste (o dentro de eps)\n",
    "        (coords[:, 0] >= lon_max - eps) |  # Cerca borde este (o dentro de eps)\n",
    "        (coords[:, 1] <= lat_min + eps) |  # Cerca borde sur (o dentro de eps)\n",
    "        (coords[:, 1] >= lat_max - eps)    # Cerca borde norte (o dentro de eps)\n",
    "    )\n",
    "\n",
    "    return is_border\n",
    "\n",
    "# Aplicar detecci√≥n de fronteras usando l√≠mites de celdas del grid\n",
    "print(f\" Identificando puntos frontera usando l√≠mites de celdas del grid...\")\n",
    "data['is_border'] = False\n",
    "\n",
    "# Asegurar que lon_bins y lat_bins est√©n disponibles (definidos en paso anterior)\n",
    "# Si no est√°n definidos, recrearlos\n",
    "if 'lon_bins' not in globals() or 'lat_bins' not in globals():\n",
    "    lon_bins = np.linspace(0, 1, n_strips + 1)\n",
    "    lat_bins = np.linspace(0, 1, n_strips + 1)\n",
    "\n",
    "for partition_id, group_idx in data.groupby('partition_id').groups.items():\n",
    "    group = data.loc[group_idx]\n",
    "    border_flags = identify_border_points(group, partition_id, lon_bins, lat_bins, eps=EPS)\n",
    "    data.loc[group_idx, 'is_border'] = border_flags\n",
    "\n",
    "# Mostrar resultados\n",
    "n_border = data['is_border'].sum()\n",
    "pct_border = 100 * n_border / len(data)\n",
    "\n",
    "print(f\" Puntos frontera identificados\")\n",
    "print(f\"  - Total puntos frontera: {n_border:,} ({pct_border:.1f}%)\")\n",
    "print(f\"  - Puntos internos: {(~data['is_border']).sum():,}\")\n",
    "\n",
    "# Crear MC Sets (Merge Candidate Sets)\n",
    "# Estos son pares de clusters que potencialmente deben fusionarse\n",
    "mc_sets = []\n",
    "\n",
    "for partition_id, group_idx in data.groupby('partition_id').groups.items():\n",
    "    group = data.loc[group_idx]\n",
    "    border_points = group[group['is_border']]\n",
    "\n",
    "    if len(border_points) > 0:\n",
    "        for idx, row in border_points.iterrows():\n",
    "            if row['local_cluster'] != -1:  # No contar noise points\n",
    "                mc_sets.append({\n",
    "                    'partition_id': partition_id,\n",
    "                    'point_id': idx,\n",
    "                    'cluster_id': row['local_cluster'],\n",
    "                    'lon': row['lon_norm'],\n",
    "                    'lat': row['lat_norm']\n",
    "                })\n",
    "\n",
    "mc_df = pd.DataFrame(mc_sets)\n",
    "print(f\"\\n MC Sets (Merge Candidates) creados\")\n",
    "print(f\"  - Total de candidatos para fusi√≥n: {len(mc_df):,}\")\n",
    "if len(mc_df) > 0:\n",
    "    print(f\"  - Clusters √∫nicos en MC: {mc_df['cluster_id'].nunique()}\")\n",
    "    print(f\"  - Particiones con candidatos: {mc_df['partition_id'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fed152c",
   "metadata": {
    "id": "1fed152c"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "# PARTE 4: FUSI√ìN DE CLUSTERS Y GLOBAL MERGE\n",
    "## Responsable: Roger\n",
    "\n",
    "### Paso 4.1: Mapeo y Generaci√≥n de IDs Globales\n",
    "\n",
    "**¬øQu√© hace este paso? (Stage 3 y 4.1 del paper)**\n",
    "\n",
    "Crea un **mapa de equivalencia** entre IDs locales y globales:\n",
    "\n",
    "1. **Busca pares para fusionar:** Usa MC Sets para encontrar clusters que deben unirse\n",
    "2. **Crea tabla de mapeo:** Tabla que dice \"cluster local (A, 5) ‚Üí cluster global 102\"\n",
    "3. **Union-Find:** Usa algoritmo eficiente para manejar transitividad\n",
    "   - Si A-B deben fusionarse y B-C deben fusionarse ‚Üí A-B-C son el mismo cluster\n",
    "\n",
    "**¬øPor qu√© es importante?**\n",
    "- Un cluster puede atravesar m√∫ltiples particiones\n",
    "- Los clusters deben tener IDs √∫nicos globales, no locales\n",
    "- La transitividad es clave: si varias particiones conectan clusters, todos deben unificarse\n",
    "\n",
    "**Ejemplo:**\n",
    "```\n",
    "Partici√≥n 0: cluster 1 (contiene puntos x, y)\n",
    "Partici√≥n 1: cluster 3 (contiene puntos y, z)\n",
    "Puntos y est√°n en ambas particiones y cercanos en espacio\n",
    "\n",
    "‚Üí Mapeo: (0, 1) ‚Üí 100, (1, 3) ‚Üí 100  (mismo ID global)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ff0f23b",
   "metadata": {
    "id": "7ff0f23b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Buscando clusters para fusionar en fronteras...\n",
      "  - Procesando 265 puntos frontera...\n",
      " Fusiones completadas: 0 pares de clusters unidos\n",
      " No se encontraron clusters para fusionar entre particiones.\n",
      "\n",
      " Creando tabla de mapeo global...\n",
      " Tabla de mapeo creada\n",
      "  - Entradas en tabla: 2\n",
      "  - Total clusters globales: 2\n",
      "\n",
      "Ejemplos del mapeo (primeros 5):\n",
      "  ('57_53', np.int64(0)) ‚Üí 0\n",
      "  ('74_42', np.int64(0)) ‚Üí 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class UnionFind:\n",
    "    \"\"\"\n",
    "    Estructura de datos Union-Find (Disjoint Set Union) para gestionar\n",
    "    equivalencias entre clusters.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.parent = {}\n",
    "        self.rank = {}\n",
    "\n",
    "    def find(self, x):\n",
    "        \"\"\"Encuentra el representante del conjunto de x (compresi√≥n de caminos)\"\"\"\n",
    "        if x not in self.parent:\n",
    "            self.parent[x] = x\n",
    "            self.rank[x] = 0\n",
    "\n",
    "        if self.parent[x] != x:\n",
    "            self.parent[x] = self.find(self.parent[x])  # Compresi√≥n de caminos\n",
    "\n",
    "        return self.parent[x]\n",
    "\n",
    "    def union(self, x, y):\n",
    "        \"\"\"Une los conjuntos que contienen x e y (uni√≥n por rango)\"\"\"\n",
    "        root_x = self.find(x)\n",
    "        root_y = self.find(y)\n",
    "\n",
    "        if root_x != root_y:\n",
    "            # Uni√≥n por rango (√°rbol m√°s peque√±o se cuelga del m√°s grande)\n",
    "            if self.rank[root_x] < self.rank[root_y]:\n",
    "                self.parent[root_x] = root_y\n",
    "            elif self.rank[root_x] > self.rank[root_y]:\n",
    "                self.parent[root_y] = root_x\n",
    "            else:\n",
    "                self.parent[root_y] = root_x\n",
    "                self.rank[root_x] += 1\n",
    "\n",
    "# Crear estructura Union-Find y manejar caso de MC Sets vac√≠o\n",
    "uf = UnionFind()\n",
    "\n",
    "# Verificar que mc_df no est√© vac√≠o antes de procesar\n",
    "print(f\" Buscando clusters para fusionar en fronteras...\")\n",
    "\n",
    "merge_count = 0\n",
    "if len(mc_df) == 0:\n",
    "    print(\"  No hay puntos frontera (MC Sets vac√≠o). No se pueden fusionar clusters entre particiones.\")\n",
    "    print(\"  Esto puede ocurrir si los clusters no cruzan fronteras de particiones.\")\n",
    "else:\n",
    "    # Dos puntos deben estar a distancia <= EPS para estar en el mismo cluster\n",
    "    dist_threshold = EPS\n",
    "    \n",
    "    # Solo comparar cada par una vez (idx_a < idx_b)\n",
    "    n_mc_points = len(mc_df)\n",
    "    print(f\"  - Procesando {n_mc_points:,} puntos frontera...\")\n",
    "    \n",
    "    for idx_a in range(n_mc_points):\n",
    "        row_a = mc_df.iloc[idx_a]\n",
    "        partition_a = row_a['partition_id']\n",
    "        \n",
    "        # Solo comparar con puntos de otras particiones\n",
    "        for idx_b in range(idx_a + 1, n_mc_points):\n",
    "            row_b = mc_df.iloc[idx_b]\n",
    "            partition_b = row_b['partition_id']\n",
    "            \n",
    "            # No comparar dentro de la misma partici√≥n\n",
    "            if partition_a == partition_b:\n",
    "                continue\n",
    "            \n",
    "            # Calcular distancia euclidiana entre puntos normalizados\n",
    "            dist = np.sqrt((row_a['lon'] - row_b['lon'])**2 +\n",
    "                           (row_a['lat'] - row_b['lat'])**2)\n",
    "            \n",
    "            # Si distancia <= EPS, los clusters deben fusionarse\n",
    "            if dist <= dist_threshold:\n",
    "                # Estos puntos est√°n cercanos: sus clusters deben fusionarse\n",
    "                # Asegurar que partition_id sea string para consistencia\n",
    "                key_a = (str(row_a['partition_id']), row_a['cluster_id'])\n",
    "                key_b = (str(row_b['partition_id']), row_b['cluster_id'])\n",
    "                \n",
    "                # Solo contar si realmente se unen\n",
    "                if uf.find(key_a) != uf.find(key_b):\n",
    "                    uf.union(key_a, key_b)\n",
    "                    merge_count += 1\n",
    "    \n",
    "    print(f\" Fusiones completadas: {merge_count} pares de clusters unidos\")\n",
    "    if merge_count == 0:\n",
    "        print(\" No se encontraron clusters para fusionar entre particiones.\")\n",
    "\n",
    "# Construir tabla de mapeo de forma m√°s robusta\n",
    "print(f\"\\n Creando tabla de mapeo global...\")\n",
    "\n",
    "mapping_table = {}\n",
    "global_id = 0\n",
    "seen_roots = {}\n",
    "\n",
    "# Asignar IDs globales basados en componentes conectados\n",
    "# Recorrer todas las particiones y sus clusters locales\n",
    "for partition_id in data['partition_id'].unique():\n",
    "    partition_data = data[data['partition_id'] == partition_id]\n",
    "    \n",
    "    # Obtener clusters locales √∫nicos (excluyendo noise)\n",
    "    local_clusters = partition_data[partition_data['local_cluster'] != -1]['local_cluster'].unique()\n",
    "    \n",
    "    for local_cluster_id in local_clusters:\n",
    "        # Asegurar que partition_id sea string para consistencia con relabeling\n",
    "        key = (str(partition_id), local_cluster_id)\n",
    "        \n",
    "        # Si el cluster no est√° en Union-Find (no se fusion√≥)\n",
    "        # crear una entrada para √©l\n",
    "        root = uf.find(key)\n",
    "        \n",
    "        if root not in seen_roots:\n",
    "            seen_roots[root] = global_id\n",
    "            global_id += 1\n",
    "        \n",
    "        mapping_table[key] = seen_roots[root]\n",
    "\n",
    "print(f\" Tabla de mapeo creada\")\n",
    "print(f\"  - Entradas en tabla: {len(mapping_table)}\")\n",
    "print(f\"  - Total clusters globales: {global_id}\")\n",
    "\n",
    "# Mostrar algunas entradas del mapeo solo si hay datos\n",
    "if len(mapping_table) > 0:\n",
    "    print(f\"\\nEjemplos del mapeo (primeros 5):\")\n",
    "    for i, (key, gid) in enumerate(list(mapping_table.items())[:5]):\n",
    "        print(f\"  {key} ‚Üí {gid}\")\n",
    "else:\n",
    "    print(f\"\\n No hay clusters para mapear (todos los puntos son noise)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142e2375",
   "metadata": {
    "id": "142e2375"
   },
   "source": [
    "\n",
    "### Paso 4.2: Relabeling Global (Reemplazar IDs Locales por Globales)\n",
    "\n",
    "**¬øQu√© hace este paso? (Stage 4.2 del paper)**\n",
    "\n",
    "Recorre todos los puntos y reemplaza sus cluster IDs locales por los globales:\n",
    "\n",
    "1. **Mapeo directo:** Para cada punto, busca su (partition_id, local_cluster) en la tabla\n",
    "2. **Asignaci√≥n:** Reemplaza el local_cluster con el global_cluster\n",
    "3. **Resultado final:** Todos los puntos tienen un cluster global √∫nico\n",
    "\n",
    "**¬øPor qu√© es importante?**\n",
    "- Despu√©s de este paso tenemos el resultado FINAL del clustering\n",
    "- Dos puntos en particiones diferentes que deber√≠an estar en el mismo cluster ahora lo est√°n\n",
    "- El algoritmo MR-DBSCAN est√° completo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13e9878d",
   "metadata": {
    "id": "13e9878d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Aplicando relabeling global a todos los puntos...\n",
      " Relabeling completado\n",
      "\n",
      "‚ïê‚ïê‚ïê RESULTADOS FINALES DEL MR-DBSCAN ‚ïê‚ïê‚ïê\n",
      "  - Total de puntos: 49,662\n",
      "  - Puntos en clusters: 2,436\n",
      "  - Puntos noise (outliers): 47,226\n",
      "  - N√∫mero total de clusters globales √∫nicos: 2\n",
      "  - ID m√°ximo de cluster global: 1\n",
      "  - Pureza del clustering: 4.9%\n",
      "\n",
      "Distribuci√≥n de clusters globales:\n",
      "  - Cluster m√°s grande: 1,281 puntos\n",
      "  - Cluster m√°s peque√±o: 1155 puntos\n",
      "  - Tama√±o promedio: 1218.0 puntos\n",
      "  - Mediana: 1218.0 puntos\n",
      "\n",
      "Primeros 10 clusters por tama√±o:\n",
      "  Cluster 1: 1,281 puntos\n",
      "  Cluster 0: 1,155 puntos\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def relabel_to_global(row, mapping_table):\n",
    "    \"\"\"\n",
    "    Convierte un cluster ID local a global usando la tabla de mapeo.\n",
    "    \n",
    "    Manejo seguro de casos donde el cluster no est√° en mapping_table\n",
    "\n",
    "    Par√°metros:\n",
    "        row: Fila del DataFrame con columnas partition_id y local_cluster\n",
    "        mapping_table: Dict con mapeo (partition_id, local_cluster) ‚Üí global_id\n",
    "    \"\"\"\n",
    "    # Si es noise, retornar -1 inmediatamente\n",
    "    if row['local_cluster'] == -1:\n",
    "        return -1\n",
    "\n",
    "    # Verificar que mapping_table no est√© vac√≠o\n",
    "    if not mapping_table:\n",
    "        print(\" Advertencia: mapping_table est√° vac√≠o. Todos los puntos ser√°n marcados como noise.\")\n",
    "        return -1\n",
    "\n",
    "    # Construir key y buscar en mapping_table\n",
    "    # Usar str() para asegurar que partition_id sea string (puede ser int)\n",
    "    key = (str(row['partition_id']), row['local_cluster'])\n",
    "\n",
    "    # Si no se encuentra el key, retornar -1 (noise)\n",
    "    # Esto puede ocurrir si hay un error en la construcci√≥n del mapeo\n",
    "    global_id = mapping_table.get(key, -1)\n",
    "    \n",
    "    if global_id == -1 and row['local_cluster'] != -1:\n",
    "        pass\n",
    "    \n",
    "    return global_id\n",
    "\n",
    "# Aplicar relabeling\n",
    "print(f\" Aplicando relabeling global a todos los puntos...\")\n",
    "data['global_cluster'] = data.apply(\n",
    "    lambda row: relabel_to_global(row, mapping_table),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(f\" Relabeling completado\")\n",
    "\n",
    "# Estad√≠sticas finales con manejo de casos edge\n",
    "n_global_clusters_points = (data['global_cluster'] != -1).sum()\n",
    "n_noise_global = (data['global_cluster'] == -1).sum()\n",
    "\n",
    "print(f\"\\n‚ïê‚ïê‚ïê RESULTADOS FINALES DEL MR-DBSCAN ‚ïê‚ïê‚ïê\")\n",
    "print(f\"  - Total de puntos: {len(data):,}\")\n",
    "print(f\"  - Puntos en clusters: {n_global_clusters_points:,}\")\n",
    "print(f\"  - Puntos noise (outliers): {n_noise_global:,}\")\n",
    "\n",
    "# Calcular n√∫mero de clusters globales de forma segura\n",
    "clusters_global = data[data['global_cluster'] != -1]['global_cluster']\n",
    "if len(clusters_global) > 0:\n",
    "    n_global_clusters_unique = clusters_global.nunique()\n",
    "    max_global_cluster_id = clusters_global.max()\n",
    "    print(f\"  - N√∫mero total de clusters globales √∫nicos: {n_global_clusters_unique}\")\n",
    "    print(f\"  - ID m√°ximo de cluster global: {max_global_cluster_id}\")\n",
    "else:\n",
    "    n_global_clusters_unique = 0\n",
    "    print(f\"  - N√∫mero total de clusters globales: 0 (todos los puntos son noise)\")\n",
    "\n",
    "if len(data) > 0:\n",
    "    purity = 100 * n_global_clusters_points / len(data)\n",
    "    print(f\"  - Pureza del clustering: {purity:.1f}%\")\n",
    "\n",
    "# Distribuci√≥n de clusters con manejo de casos vac√≠os\n",
    "if len(clusters_global) > 0:\n",
    "    cluster_sizes = clusters_global.value_counts()\n",
    "    print(f\"\\nDistribuci√≥n de clusters globales:\")\n",
    "    print(f\"  - Cluster m√°s grande: {cluster_sizes.max():,} puntos\")\n",
    "    print(f\"  - Cluster m√°s peque√±o: {cluster_sizes.min()} puntos\")\n",
    "    print(f\"  - Tama√±o promedio: {cluster_sizes.mean():.1f} puntos\")\n",
    "    print(f\"  - Mediana: {cluster_sizes.median():.1f} puntos\")\n",
    "    \n",
    "    print(f\"\\nPrimeros 10 clusters por tama√±o:\")\n",
    "    for cluster_id, size in cluster_sizes.head(10).items():\n",
    "        print(f\"  Cluster {int(cluster_id)}: {size:,} puntos\")\n",
    "else:\n",
    "    print(f\"\\n No hay clusters para mostrar (todos los puntos son noise)\")\n",
    "    cluster_sizes = pd.Series(dtype=int)  # Crear Series vac√≠o para evitar errores despu√©s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f40d14",
   "metadata": {
    "id": "84f40d14"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "# PARTE 5: VALIDACI√ìN Y ALMACENAMIENTO DE RESULTADOS\n",
    "\n",
    "### Paso 5.1: Guardar Resultados\n",
    "\n",
    "**¬øQu√© hace este paso?**\n",
    "\n",
    "Almacena los resultados finales en archivos para:\n",
    "- Documentaci√≥n y auditor√≠a\n",
    "- Visualizaci√≥n posterior\n",
    "- Comparaci√≥n con resultados esperados\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fce8fb1",
   "metadata": {
    "id": "8fce8fb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Resultados guardados en: mr_dbscan_results.csv\n",
      "  - Columnas: lon, lat, lon_norm, lat_norm, lon_bin, lat_bin, partition_id, local_cluster, is_border, global_cluster\n",
      " Resumen guardado en: clustering_summary.csv\n",
      " Reporte guardado en: mr_dbscan_report.txt\n",
      "\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "          REPORTE FINAL DEL MR-DBSCAN CHALLENGE\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "\n",
      "CONFIGURACI√ìN DEL ALGORITMO:\n",
      "  - Eps (epsilon): 0.002\n",
      "  - MinPts (m√≠nimo de puntos): 1000\n",
      "  - N√∫mero de particiones (strips): 120\n",
      "\n",
      "DATOS DE ENTRADA:\n",
      "  - Total de puntos: 49,662\n",
      "  - Rango Longitud: [121.148300, 121.866100]\n",
      "  - Rango Latitud: [30.886600, 31.999100]\n",
      "\n",
      "STAGE 1 - PARTICIONAMIENTO:\n",
      "  - N√∫mero total de particiones con datos: 1873\n",
      "  - Puntos por partici√≥n (promedio): 26.5\n",
      "\n",
      "STAGE 2 - DBSCAN LOCAL:\n",
      "  - Clusters locales identificados: 1\n",
      "  - Puntos en clusters locales: 2,436\n",
      "  - Puntos noise/outliers locales: 47,226\n",
      "\n",
      "STAGE 3 - DETECCI√ìN DE CRUCES:\n",
      "  - Puntos frontera identificados: 34,814\n",
      "  - MC Sets (candidatos para fusi√≥n): 265\n",
      "  - Pares de clusters fusionados: 0\n",
      "\n",
      "STAGE 4 - FUSI√ìN GLOBAL:\n",
      "  - Clusters globales finales: 2\n",
      "  - Puntos en clusters globales: 2,436\n",
      "  - Puntos noise finales: 47,226\n",
      "\n",
      "CALIDAD DE CLUSTERING:\n",
      "  - Pureza (% puntos en clusters): 4.9%\n",
      "  - Tama√±o m√°ximo de cluster: 1,281 puntos\n",
      "  - Tama√±o m√≠nimo de cluster: 1155 puntos\n",
      "  - Tama√±o promedio: 1218.0 puntos\n",
      "  - Desviaci√≥n est√°ndar: 89.1\n",
      "\n",
      "ARCHIVOS GENERADOS:\n",
      "  - mr_dbscan_results.csv\n",
      "  - clustering_summary.csv\n",
      "  - mr_dbscan_report.txt\n",
      "\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Guardar resultados completos a CSV\n",
    "output_file = 'mr_dbscan_results.csv'\n",
    "data.to_csv(output_file, index=False)\n",
    "print(f\" Resultados guardados en: {output_file}\")\n",
    "print(f\"  - Columnas: {', '.join(data.columns.tolist())}\")\n",
    "\n",
    "# Guardar resumen de clustering\n",
    "summary_data = data[['lon', 'lat', 'global_cluster']].copy()\n",
    "summary_file = 'clustering_summary.csv'\n",
    "summary_data.to_csv(summary_file, index=False)\n",
    "print(f\" Resumen guardado en: {summary_file}\")\n",
    "\n",
    "# Crear reporte de estad√≠sticas con manejo seguro de variables\n",
    "# Verificar que todas las variables est√©n definidas antes de usarlas\n",
    "try:\n",
    "    # Obtener estad√≠sticas de clusters locales\n",
    "    clusters_local_unique = data[data['local_cluster'] != -1]['local_cluster'].nunique() if len(data[data['local_cluster'] != -1]) > 0 else 0\n",
    "    n_local_clustered = (data['local_cluster'] != -1).sum()\n",
    "    n_local_noise = (data['local_cluster'] == -1).sum()\n",
    "    \n",
    "    # Obtener estad√≠sticas de clusters globales\n",
    "    clusters_global_unique = data[data['global_cluster'] != -1]['global_cluster'].nunique() if len(data[data['global_cluster'] != -1]) > 0 else 0\n",
    "    n_global_clustered = (data['global_cluster'] != -1).sum()\n",
    "    n_global_noise = (data['global_cluster'] == -1).sum()\n",
    "    \n",
    "    # Estad√≠sticas de distribuci√≥n de clusters\n",
    "    if len(data[data['global_cluster'] != -1]) > 0:\n",
    "        cluster_sizes_report = data[data['global_cluster'] != -1]['global_cluster'].value_counts()\n",
    "        max_cluster_size = cluster_sizes_report.max()\n",
    "        min_cluster_size = cluster_sizes_report.min()\n",
    "        mean_cluster_size = cluster_sizes_report.mean()\n",
    "        std_cluster_size = cluster_sizes_report.std()\n",
    "    else:\n",
    "        max_cluster_size = 0\n",
    "        min_cluster_size = 0\n",
    "        mean_cluster_size = 0.0\n",
    "        std_cluster_size = 0.0\n",
    "    \n",
    "    # MC Sets\n",
    "    mc_sets_count = len(mc_df) if 'mc_df' in globals() and len(mc_df) > 0 else 0\n",
    "    \n",
    "    # Pureza\n",
    "    purity_pct = (100 * n_global_clustered / len(data)) if len(data) > 0 else 0.0\n",
    "    \n",
    "    # Promedio de puntos por partici√≥n\n",
    "    avg_points_per_partition = (len(data) / n_partitions) if n_partitions > 0 else 0.0\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\" Error al preparar estad√≠sticas para el reporte: {e}\")\n",
    "    # Valores por defecto en caso de error\n",
    "    clusters_local_unique = 0\n",
    "    n_local_clustered = 0\n",
    "    n_local_noise = len(data)\n",
    "    clusters_global_unique = 0\n",
    "    n_global_clustered = 0\n",
    "    n_global_noise = len(data)\n",
    "    max_cluster_size = 0\n",
    "    min_cluster_size = 0\n",
    "    mean_cluster_size = 0.0\n",
    "    std_cluster_size = 0.0\n",
    "    mc_sets_count = 0\n",
    "    purity_pct = 0.0\n",
    "    avg_points_per_partition = 0.0\n",
    "\n",
    "report_file = 'mr_dbscan_report.txt'\n",
    "\n",
    "# Crear reporte\n",
    "report = f\"\"\"\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "          REPORTE FINAL DEL MR-DBSCAN CHALLENGE\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "CONFIGURACI√ìN DEL ALGORITMO:\n",
    "  - Eps (epsilon): {EPS}\n",
    "  - MinPts (m√≠nimo de puntos): {MIN_SAMPLES}\n",
    "  - N√∫mero de particiones (strips): {n_strips}\n",
    "\n",
    "DATOS DE ENTRADA:\n",
    "  - Total de puntos: {len(data):,}\n",
    "  - Rango Longitud: [{data['lon'].min():.6f}, {data['lon'].max():.6f}]\n",
    "  - Rango Latitud: [{data['lat'].min():.6f}, {data['lat'].max():.6f}]\n",
    "\n",
    "STAGE 1 - PARTICIONAMIENTO:\n",
    "  - N√∫mero total de particiones con datos: {n_partitions}\n",
    "  - Puntos por partici√≥n (promedio): {avg_points_per_partition:.1f}\n",
    "\n",
    "STAGE 2 - DBSCAN LOCAL:\n",
    "  - Clusters locales identificados: {clusters_local_unique}\n",
    "  - Puntos en clusters locales: {n_local_clustered:,}\n",
    "  - Puntos noise/outliers locales: {n_local_noise:,}\n",
    "\n",
    "STAGE 3 - DETECCI√ìN DE CRUCES:\n",
    "  - Puntos frontera identificados: {data['is_border'].sum():,}\n",
    "  - MC Sets (candidatos para fusi√≥n): {mc_sets_count:,}\n",
    "  - Pares de clusters fusionados: {merge_count if 'merge_count' in globals() else 0}\n",
    "\n",
    "STAGE 4 - FUSI√ìN GLOBAL:\n",
    "  - Clusters globales finales: {clusters_global_unique}\n",
    "  - Puntos en clusters globales: {n_global_clustered:,}\n",
    "  - Puntos noise finales: {n_global_noise:,}\n",
    "\n",
    "CALIDAD DE CLUSTERING:\n",
    "  - Pureza (% puntos en clusters): {purity_pct:.1f}%\n",
    "  - Tama√±o m√°ximo de cluster: {max_cluster_size:,} puntos\n",
    "  - Tama√±o m√≠nimo de cluster: {min_cluster_size} puntos\n",
    "  - Tama√±o promedio: {mean_cluster_size:.1f} puntos\n",
    "  - Desviaci√≥n est√°ndar: {std_cluster_size:.1f}\n",
    "\n",
    "ARCHIVOS GENERADOS:\n",
    "  - {output_file}\n",
    "  - {summary_file}\n",
    "  - {report_file}\n",
    "\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\"\"\"\n",
    "with open(report_file, 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(f\" Reporte guardado en: {report_file}\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db79ec57",
   "metadata": {
    "id": "db79ec57"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Referencias y Bibliograf√≠a\n",
    "\n",
    "1. **Paper Original:**\n",
    "   - He, Y., Tan, H., Luo, W., et al. (2011). \"MR-DBSCAN: An Efficient Parallel Density-based Clustering Algorithm using MapReduce\"\n",
    "\n",
    "2. **Algoritmo DBSCAN:**\n",
    "   - Ester, M., Kriegel, H. P., Sander, J., & Xu, X. (1996). \"A density-based algorithm for discovering clusters in large spatial databases\"\n",
    "\n",
    "3. **MapReduce:**\n",
    "   - Dean, J., & Ghemawat, S. (2004). \"MapReduce: Simplified Data Processing on Large Clusters\"\n",
    "\n",
    "4. **Herramientas Utilizadas:**\n",
    "   - Hadoop: https://hadoop.apache.org/\n",
    "   - PySpark: https://spark.apache.org/docs/latest/api/python/\n",
    "   - Scikit-Learn: https://scikit-learn.org/stable/\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
